# AI_service_LLM/chatbot/core/llm.py

from __future__ import annotations

import os
from typing import Optional, List, Dict, Any

from openai import OpenAI

# ============================================
# ðŸ”¹ OpenAI í´ë¼ì´ì–¸íŠ¸ & ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
# ============================================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHATBOT_MODEL = os.getenv("CHATBOT_MODEL", "gpt-4o-mini")

_client: Optional[OpenAI] = None


def get_client() -> OpenAI:
    global _client
    if _client is None:
        if not OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        _client = OpenAI(api_key=OPENAI_API_KEY)
    return _client


# ============================================
# ðŸ”¹ ê³µí†µ LLM í˜¸ì¶œ í•¨ìˆ˜
# ============================================

def call_llm(
    system_prompt: str,
    user_message: str,
    context: str | None = None,
    model: str | None = None,
    temperature: float = 0.2,
) -> str:
    """
    ì—ì´ì „íŠ¸ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” LLM í˜¸ì¶œ í•¨ìˆ˜.

    - system_prompt: ì‹œìŠ¤í…œ ì—­í•  ì„¤ëª… (ì—ì´ì „íŠ¸ë³„ í”„ë¡¬í”„íŠ¸)
    - user_message: ì‚¬ìš©ìžì˜ ì‹¤ì œ ì§ˆë¬¸
    - context: RAGë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ (ì„ íƒ)
    """
    messages: List[Dict[str, str]] = [
        {"role": "system", "content": system_prompt},
    ]

    if context:
        messages.append(
            {
                "role": "user",
                "content": f"[ê²€ìƒ‰ëœ ì°¸ê³  ì •ë³´]\n{context}",
            }
        )

    messages.append(
        {
            "role": "user",
            "content": user_message,
        }
    )

    client = get_client()
    resp = client.chat.completions.create(
        model=model or CHATBOT_MODEL,
        messages=messages,
        temperature=temperature,
    )

    return resp.choices[0].message.content or ""


# run_llm ì´ë¼ëŠ” ì´ë¦„ì„ ì“°ëŠ” ì½”ë“œë„ ìžˆì„ ìˆ˜ ìžˆìœ¼ë‹ˆ alias ì œê³µ
def run_llm(
    system_prompt: str,
    user_message: str,
    context: str | None = None,
    model: str | None = None,
    temperature: float = 0.2,
) -> str:
    return call_llm(
        system_prompt=system_prompt,
        user_message=user_message,
        context=context,
        model=model,
        temperature=temperature,
    )
